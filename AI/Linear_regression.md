### ## TIL: 선형 회귀 모델의 최적값 탐색 방법

#### **1. 기본 개념**

* **머신러닝 (Machine Learning)**: 데이터를 기반으로 최적의 **수학 모델(함수)**을 만들어, 새로운 데이터에 대한 예측이나 분류를 수행하는 방법.
* **모델 (Model)**: 입력 데이터(x)와 예측 결과(ŷ) 사이의 관계를 나타내는 **수학식(함수)**. 예를 들어 선형 회귀 모델은 `ŷ = ax + b`임.
* **학습 (Training)**: 모델 학습의 목표는 주어진 데이터를 가장 잘 설명하는 최적의 파라미터(선형 회귀에서는 기울기 `a`와 절편 `b`)를 찾는 과정임.

---

#### **2. 선형 회귀의 해법: 최적의 a, b를 찾는 4가지 방법**

`y = ax + b` 모델에서 데이터에 가장 잘 맞는 `a`와 `b`를 찾는 방법은 여러 가지가 있음.

* **1. Grid Search (그리드 서치)**
    * **개념**: 정해진 범위 내에서 가능한 모든 `a`, `b` 조합을 시도해보고, 오차(Error)가 가장 작은 조합을 선택하는 방식.
    * **장점**: 원리가 단순하고 직관적임.
    * **단점**: 매우 비효율적이며, 파라미터가 많아지면 현실적으로 사용 불가능함.

* **2. 정규방정식 (Normal Equation)**
    * **개념**: 비용 함수(MSE)를 최소화하는 `a`, `b`를 **수학 공식을 통해 한 번에 계산**하는 방법. (미분과 선형대수 활용)
    * **장점**: 반복 없이 정확한 해를 한 번에 구할 수 있음.
    * **단점**: 데이터의 특성(피처)이 매우 많아지면 **계산이 극도로 느려지고 메모리를 많이 사용함.** 역행렬 계산이 불가능한 경우도 있음.

* **3. 경사 하강법 (Gradient Descent, GD)**
    * **개념**: 비용 함수의 경사(기울기)를 따라 **조금씩 점진적으로** 오차가 가장 낮은 지점으로 이동하는 반복적인 방법.
    * **장점**: 데이터가 매우 커도 효율적으로 동작하여 정규방정식의 단점을 보완함.
    * **단점**: 잘못된 시작점이나 복잡한 함수 형태 때문에 **전체 최솟값(Global Minimum)이 아닌 지역 최솟값(Local Minimum)에 빠질 위험**이 있음. 학습률(learning rate) 설정이 중요함.

* **4. 아담 (Adam)**
    * **개념**: 기본 경사 하강법의 개선된 버전. 이동하던 **관성(Momentum)을 고려**하고, 상황에 맞게 **학습률을 자동으로 조절**하여 더 빠르고 안정적으로 최솟값을 탐색함.
    * **장점**: 대부분의 경우 기본 경사 하강법보다 빠르고 안정적인 성능을 보임. 현재 딥러닝에서 가장 널리 사용되는 최적화 방법 중 하나임.
